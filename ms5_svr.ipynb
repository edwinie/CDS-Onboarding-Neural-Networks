{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. How does an SVR work? \n",
    "\n",
    "Support Vector Regression predicts continuous outcomes by fitting a hyperplane within an acceptable margin, penalizing errors only beyond that boundary. It uses kernel functions to transform data into higher dimensions, enabling it to capture nonlinear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. How is this similar to or different from linear regression? What do \n",
    "the different kernel types between linear, polynomial, and radial \n",
    "basis function (RBF) do? \n",
    "\n",
    "SVR resembles linear regression by predicting continuous values but differs by penalizing only errors exceeding a certain margin, rather than all deviations. Kernels distinguish SVR from linear regression: linear kernels model straightforward linear trends, polynomial kernels capture complex interactions, and radial basis function kernels handle intricate nonlinear patterns through similarity measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. For each kernel, what happens when you increase or decrease the \n",
    "magnitudes of hyperparameters C and gamma? Why? Justify with \n",
    "plots. \n",
    "\n",
    "Linear Kernel: \n",
    "The C parameter in linear kernels controls the trade-off between fitting the training data and maintaining a large margin. Increasing C reduces regularization, allowing the model to fit training data more closely but risking overfitting. Lower C values enforce more regularization, creating smoother decision boundaries that may underfit the data. Linear kernels are not affected by the gamma parameter.\n",
    "\n",
    "RBF Kernel:\n",
    "The C parameter in RBF kernels functions similarly to linear kernels, balancing fit versus regularization. The gamma parameter controls the influence radius of each training example. Lower gamma values create smoother decision boundaries as each example affects a wider area, potentially causing underfitting. Higher gamma values restrict each example's influence to nearby points, creating more complex boundaries that closely follow training data and may overfit. RBF kernels show the most dramatic performance changes with gamma variations.\n",
    "\n",
    "Polynomial Kernel:\n",
    "For polynomial kernels, C similarly balances fitting versus regularization. The gamma parameter influences the curvature and complexity of the decision boundary. Higher gamma values create more curved, complex boundaries that may overfit the data. Polynomial kernels generally show less sensitivity to extreme gamma values compared to RBF kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"Top_spotify_songs.csv\")\n",
    "features = [\"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\", \n",
    "           \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"]\n",
    "target = \"popularity\"\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "c_values = [0.1, 1, 10, 100]\n",
    "gamma_values = [0.01, 0.1, 1, 10]\n",
    "\n",
    "def evaluate_model(kernel, C, gamma=None):\n",
    "    if kernel == 'linear':\n",
    "        svr = SVR(kernel=kernel, C=C)\n",
    "    else:\n",
    "        svr = SVR(kernel=kernel, C=C, gamma=gamma)\n",
    "    \n",
    "    svr.fit(X_train_scaled, y_train)\n",
    "    train_pred = svr.predict(X_train_scaled)\n",
    "    val_pred = svr.predict(X_val_scaled)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, train_pred)\n",
    "    val_mse = mean_squared_error(y_val, val_pred)\n",
    "    \n",
    "    return train_mse, val_mse\n",
    "\n",
    "linear_results = [evaluate_model('linear', c) for c in c_values]\n",
    "linear_train_mse, linear_val_mse = zip(*linear_results)\n",
    "\n",
    "rbf_c_results = [evaluate_model('rbf', c, 0.1) for c in c_values]\n",
    "rbf_c_train_mse, rbf_c_val_mse = zip(*rbf_c_results)\n",
    "\n",
    "rbf_gamma_results = [evaluate_model('rbf', 1, g) for g in gamma_values]\n",
    "rbf_gamma_train_mse, rbf_gamma_val_mse = zip(*rbf_gamma_results)\n",
    "\n",
    "poly_c_results = [evaluate_model('poly', c, 0.1) for c in c_values]\n",
    "poly_c_train_mse, poly_c_val_mse = zip(*poly_c_results)\n",
    "\n",
    "poly_gamma_results = [evaluate_model('poly', 1, g) for g in gamma_values]\n",
    "poly_gamma_train_mse, poly_gamma_val_mse = zip(*poly_gamma_results)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Linear Kernel - C effect\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(c_values, linear_train_mse, 'o-', label='Training MSE')\n",
    "plt.plot(c_values, linear_val_mse, 'o--', label='Validation MSE')\n",
    "plt.xscale('log')\n",
    "plt.title('Linear Kernel: C effect')\n",
    "plt.xlabel('C value')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "# RBF Kernel - C effect\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(c_values, rbf_c_train_mse, 'o-', label='Training MSE')\n",
    "plt.plot(c_values, rbf_c_val_mse, 'o--', label='Validation MSE')\n",
    "plt.xscale('log')\n",
    "plt.title('RBF Kernel: C effect (gamma=0.1)')\n",
    "plt.xlabel('C value')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "# RBF Kernel - gamma effect\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(gamma_values, rbf_gamma_train_mse, 'o-', label='Training MSE')\n",
    "plt.plot(gamma_values, rbf_gamma_val_mse, 'o--', label='Validation MSE')\n",
    "plt.xscale('log')\n",
    "plt.title('RBF Kernel: gamma effect (C=1)')\n",
    "plt.xlabel('gamma value')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "# Poly Kernel - C effect\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(c_values, poly_c_train_mse, 'o-', label='Training MSE')\n",
    "plt.plot(c_values, poly_c_val_mse, 'o--', label='Validation MSE')\n",
    "plt.xscale('log')\n",
    "plt.title('Polynomial Kernel: C effect (gamma=0.1)')\n",
    "plt.xlabel('C value')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "# Poly Kernel - gamma effect\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(gamma_values, poly_gamma_train_mse, 'o-', label='Training MSE')\n",
    "plt.plot(gamma_values, poly_gamma_val_mse, 'o--', label='Validation MSE')\n",
    "plt.xscale('log')\n",
    "plt.title('Polynomial Kernel: gamma effect (C=1)')\n",
    "plt.xlabel('gamma value')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
